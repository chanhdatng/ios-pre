{
  "topic": "visionos",
  "title": "Vision Pro & Spatial Computing (visionOS)",
  "description": "Emerging platform - increasingly asked at Apple and forward-thinking companies",
  "cards": [
    {
      "id": "vision_001",
      "front": "How would you design a user interface for visionOS, considering spatial interactions (eye-tracking, hand gestures, voice)? How is this fundamentally different from iOS 2D interface design?",
      "back": "**visionOS UI Design Principles:**\n\n**Fundamental Differences:**\n\n| iOS 2D | visionOS Spatial |\n|--------|------------------|\n| Touch targets (44pt min) | Gaze targets (60pt min) |\n| Fixed screen bounds | Infinite canvas |\n| 2D layers | 3D depth & volumes |\n| Direct touch | Indirect (look + pinch) |\n| Single focus point | Peripheral awareness |\n\n**Input Hierarchy:**\n```\n1. Eyes (gaze) → Hover/highlight\n2. Hands (pinch) → Select/tap\n3. Hands (drag) → Move/scroll\n4. Voice (Siri) → Commands\n```\n\n**SwiftUI for visionOS:**\n```swift\nimport SwiftUI\nimport RealityKit\n\nstruct ContentView: View {\n    var body: some View {\n        // Window - 2D content floating in space\n        NavigationStack {\n            List {\n                ForEach(items) { item in\n                    ItemRow(item: item)\n                        .hoverEffect()  // Essential for gaze feedback\n                }\n            }\n        }\n        .ornament(attachmentAnchor: .scene(.bottom)) {\n            // Ornaments float outside window\n            ToolbarView()\n        }\n    }\n}\n\n// Volumetric content\nstruct VolumetricView: View {\n    var body: some View {\n        RealityView { content in\n            let model = try! await Entity(named: \"Scene\")\n            content.add(model)\n        }\n        .gesture(TapGesture().targetedToAnyEntity().onEnded { value in\n            // Handle tap on 3D object\n        })\n    }\n}\n```\n\n**Key Design Patterns:**\n\n**1. Hover Effects (Critical!)**\n```swift\n// Users need gaze feedback\nButton(\"Action\") { }\n    .hoverEffect()  // Built-in highlight on gaze\n    \n// Custom hover\n.hoverEffect { effect, isActive, proxy in\n    effect.scaleEffect(isActive ? 1.1 : 1.0)\n}\n```\n\n**2. Depth & Layering**\n```swift\n// Use Z-offset for visual hierarchy\nZStack {\n    BackgroundView()\n        .offset(z: -50)  // Further back\n    ContentView()\n        .offset(z: 0)    // Default plane\n    OverlayView()\n        .offset(z: 50)   // Closer to user\n}\n```\n\n**3. Ergonomic Placement**\n```swift\n// Content should be:\n// - At eye level or slightly below\n// - 1-2 meters away\n// - Not requiring head movement to scan\n\n.defaultWorldScaling(.dynamic) // Adapts to user distance\n```\n\n**Anti-Patterns:**\n- Tiny touch targets (impossible to gaze-select)\n- Rapid animations (causes discomfort)\n- Content requiring looking up (neck strain)\n- Cluttered UI (overwhelming in spatial context)",
      "tags": ["visionos", "spatial-ui", "eye-tracking", "gestures", "swiftui"],
      "sources": ["https://developer.apple.com/design/human-interface-guidelines/designing-for-visionos"],
      "summary": "Gaze + pinch interaction, larger targets (60pt), hover effects critical, depth layering, ergonomic placement (eye level, 1-2m away)."
    },
    {
      "id": "vision_002",
      "front": "In visionOS, how do you handle 3D layout and spatial widgets? How would you adapt an existing iOS app to visionOS while maintaining code reuse?",
      "back": "**Adapting iOS Apps to visionOS:**\n\n**Shared Code Strategy:**\n```swift\n// MARK: - Platform-Agnostic Core\n// ViewModels, business logic, networking - shared\nclass ProductViewModel: ObservableObject {\n    @Published var products: [Product] = []\n    \n    func loadProducts() async {\n        products = try await api.fetchProducts()\n    }\n}\n\n// MARK: - Platform-Specific Views\n#if os(visionOS)\nstruct ProductListView: View {\n    @StateObject var viewModel = ProductViewModel()\n    \n    var body: some View {\n        // Spatial layout for Vision Pro\n        HStack(spacing: 100) {\n            ForEach(viewModel.products) { product in\n                ProductCard3D(product: product)\n                    .frame(depth: 50)\n            }\n        }\n        .ornament(attachmentAnchor: .scene(.leading)) {\n            FilterPanel()\n        }\n    }\n}\n#else\nstruct ProductListView: View {\n    @StateObject var viewModel = ProductViewModel()\n    \n    var body: some View {\n        // Traditional 2D list for iOS\n        List(viewModel.products) { product in\n            ProductRow(product: product)\n        }\n    }\n}\n#endif\n```\n\n**3D Layout Containers:**\n```swift\n// visionOS-specific layouts\nstruct SpatialGallery: View {\n    var body: some View {\n        // Horizontal orbital layout\n        GeometryReader3D { proxy in\n            ForEach(0..<items.count, id: \\.self) { index in\n                let angle = Double(index) / Double(items.count) * 360\n                ItemView(item: items[index])\n                    .rotation3DEffect(\n                        .degrees(angle),\n                        axis: (x: 0, y: 1, z: 0)\n                    )\n                    .offset(z: 200)  // Radius of orbit\n            }\n        }\n    }\n}\n\n// Volumetric window\n@main\nstruct MyApp: App {\n    var body: some Scene {\n        // Standard 2D window\n        WindowGroup {\n            ContentView()\n        }\n        \n        // 3D volumetric content\n        WindowGroup(id: \"3d-viewer\") {\n            VolumetricProductView()\n        }\n        .windowStyle(.volumetric)\n        .defaultSize(width: 0.5, height: 0.5, depth: 0.5, in: .meters)\n        \n        // Immersive space\n        ImmersiveSpace(id: \"showroom\") {\n            ShowroomView()\n        }\n    }\n}\n```\n\n**Progressive Enhancement:**\n```swift\nstruct AdaptiveProductView: View {\n    let product: Product\n    \n    var body: some View {\n        Group {\n            #if os(visionOS)\n            // Rich 3D model on Vision Pro\n            Model3D(named: product.modelName)\n                .frame(depth: 100)\n            #else\n            // Fallback image on iOS\n            AsyncImage(url: product.imageURL)\n            #endif\n        }\n    }\n}\n```\n\n**Code Reuse Statistics:**\n- **90%+ shared:** ViewModels, networking, data models\n- **70%+ shared:** SwiftUI views (with minor adaptations)\n- **Platform-specific:** Spatial layouts, 3D interactions, immersive scenes",
      "tags": ["visionos", "ios-migration", "code-reuse", "3d-layout", "cross-platform"],
      "sources": ["https://developer.apple.com/documentation/visionos/bringing-your-app-to-visionos"],
      "summary": "Share ViewModels/logic (90%+). Platform-specific views with #if os(visionOS). Use WindowGroup for 2D, .volumetric for 3D, ImmersiveSpace for full immersion."
    },
    {
      "id": "vision_003",
      "front": "What are the privacy and safety considerations for an app that uses eye-tracking data? How would you design responsible eye-tracking features?",
      "back": "**Eye-Tracking Privacy & Safety:**\n\n**Privacy Concerns:**\n1. **Attention patterns** reveal interests, reading habits\n2. **Gaze duration** indicates engagement, preferences\n3. **Pupil dilation** can indicate emotional state\n4. **Medical inference** - eye movement patterns can indicate conditions\n\n**visionOS Privacy Model:**\n```swift\n// Apps NEVER get raw eye-tracking data!\n// System handles gaze → hover effect\n// Apps only receive: \"user tapped this element\"\n\n// WRONG - This doesn't exist\nlet gazePosition = eyeTracker.currentPosition  // ❌ Not available\n\n// RIGHT - System abstraction\nButton(\"Buy Now\") { \n    // You only know they selected this\n    // NOT where they looked before\n}\n.hoverEffect()  // System handles hover, not you\n```\n\n**What Apps CAN Access:**\n```swift\n// 1. Tap/selection events (after user commits)\nonTapGesture { location in\n    // Final selection point only\n}\n\n// 2. Hover state (boolean, not coordinates)\n.onHover { isHovering in\n    // true/false, not gaze coordinates\n}\n\n// 3. ARKit hand tracking (with permission)\nlet handAnchor = handTracking.latestAnchors.first\n```\n\n**Responsible Design Principles:**\n\n**1. Avoid Inferred Attention Tracking**\n```swift\n// BAD: Inferring attention from hover timing\nclass AttentionTracker {\n    var hoverStartTime: Date?\n    \n    func onHover(_ isHovering: Bool, item: Item) {\n        if isHovering {\n            hoverStartTime = Date()\n        } else {\n            let duration = Date().timeIntervalSince(hoverStartTime!)\n            analytics.track(\"attention\", duration: duration, item: item)  // ❌ Creepy\n        }\n    }\n}\n\n// GOOD: Only track explicit actions\nfunc onItemSelected(_ item: Item) {\n    analytics.track(\"item_selected\", item: item)  // ✓ User action only\n}\n```\n\n**2. Clear Feedback for Gaze Actions**\n```swift\n// Users should always know when gaze matters\nstruct GazeButton: View {\n    @State private var isHovered = false\n    \n    var body: some View {\n        Button(action: action) {\n            Label(title, systemImage: icon)\n        }\n        .hoverEffect(.highlight)  // Clear visual feedback\n        .onHover { isHovered = $0 }\n        .overlay {\n            if isHovered {\n                // Additional feedback that gaze is detected\n                RoundedRectangle(cornerRadius: 8)\n                    .stroke(Color.blue, lineWidth: 2)\n            }\n        }\n    }\n}\n```\n\n**3. Avoid Gaze-Triggered Side Effects**\n```swift\n// BAD: Auto-play video on gaze\n.onHover { isHovering in\n    if isHovering {\n        videoPlayer.play()  // ❌ Unexpected\n    }\n}\n\n// GOOD: Require explicit selection\n.onTapGesture {\n    videoPlayer.play()  // ✓ User chose to play\n}\n```\n\n**4. Safety: Prevent Eye Strain**\n```swift\n// Limit rapid visual changes\nwithAnimation(.easeInOut(duration: 0.3)) {  // Smooth, not jarring\n    // state change\n}\n\n// Comfortable viewing distances\n.frame(depth: 100)  // Not too close\n.position(z: -200)  // Not requiring eye convergence\n```\n\n**Privacy Best Practices Summary:**\n- Never infer preferences from hover patterns\n- Only track explicit user actions\n- Provide clear gaze feedback\n- Don't trigger actions on gaze alone\n- Respect system privacy abstractions",
      "tags": ["visionos", "eye-tracking", "privacy", "safety", "ethics"],
      "sources": ["https://developer.apple.com/design/human-interface-guidelines/eyes"],
      "summary": "Apps never get raw gaze data - system abstracts to hover/tap events. Don't track hover duration. Only log explicit selections. Provide clear feedback."
    },
    {
      "id": "vision_004",
      "front": "Describe a compelling application for Apple Vision Pro in healthcare, education, or enterprise. What are the key technical challenges?",
      "back": "**Vision Pro Healthcare Application: Surgical Planning**\n\n**Concept: 3D Surgical Planning & Training**\n```swift\n// View patient anatomy in 3D before surgery\nstruct SurgicalPlanningApp: App {\n    var body: some Scene {\n        // Standard 2D for patient info\n        WindowGroup(\"Patient Data\") {\n            PatientInfoView()\n        }\n        \n        // 3D anatomical visualization\n        ImmersiveSpace(id: \"anatomy-viewer\") {\n            AnatomyVisualizationView()\n        }\n        .immersionStyle(selection: .constant(.mixed), in: .mixed)\n    }\n}\n\nstruct AnatomyVisualizationView: View {\n    @State private var selectedOrgan: Organ?\n    \n    var body: some View {\n        RealityView { content in\n            // Load patient's CT/MRI scan as 3D model\n            let anatomy = try! await loadPatientAnatomy()\n            content.add(anatomy)\n        } update: { content in\n            // Highlight selected organ\n            if let organ = selectedOrgan {\n                highlightOrgan(organ, in: content)\n            }\n        }\n        .gesture(\n            TapGesture().targetedToAnyEntity()\n                .onEnded { value in\n                    selectedOrgan = identifyOrgan(value.entity)\n                }\n        )\n    }\n}\n```\n\n**Technical Challenges:**\n\n**1. Medical Data Rendering**\n```swift\n// Challenge: CT/MRI data is volumetric, not mesh\n// Solution: Volume rendering in RealityKit\n\nfunc loadVolumetricData(_ dicomFiles: [URL]) async throws -> Entity {\n    // Convert DICOM to 3D texture\n    let volumeTexture = try await processVolumetricData(dicomFiles)\n    \n    // Custom shader for volume rendering\n    var material = CustomMaterial()\n    material.custom.texture = volumeTexture\n    \n    let entity = Entity()\n    entity.components.set(ModelComponent(mesh: .generateBox(size: 0.3), materials: [material]))\n    return entity\n}\n```\n\n**2. HIPAA Compliance**\n```swift\n// Challenge: Patient data privacy\nstruct SecureAnatomyLoader {\n    func loadPatientData(patientId: String) async throws -> AnatomyModel {\n        // Data must stay on-device or use encrypted transmission\n        guard let localPath = LocalVault.path(for: patientId) else {\n            throw SecurityError.dataNotAvailableLocally\n        }\n        \n        // Never send to cloud without consent\n        return try await loadFromSecureStorage(localPath)\n    }\n}\n\n// Audit logging required\nfunc accessPatientRecord(_ id: String, user: Surgeon) {\n    AuditLog.record(\n        action: .viewedRecord,\n        patientId: id,\n        accessedBy: user.id,\n        timestamp: Date(),\n        purpose: .surgicalPlanning\n    )\n}\n```\n\n**3. Precision Interaction**\n```swift\n// Challenge: Surgical planning needs millimeter accuracy\n// Solution: Multi-modal input + precision mode\n\nstruct PrecisionTool: View {\n    @State private var measurementMode = false\n    \n    var body: some View {\n        RealityView { content in\n            // Load surgical instruments as overlays\n        }\n        .gesture(\n            MagnifyGesture()\n                .onChanged { value in\n                    // Precise scaling for measurement\n                    currentScale = value.magnification\n                }\n        )\n        // Voice commands for hands-free operation\n        .onVoiceCommand(\"Measure\") {\n            measurementMode = true\n        }\n    }\n}\n```\n\n**4. Real-time Collaboration**\n```swift\n// Challenge: Multiple surgeons viewing same model\nclass CollaborativeSession: ObservableObject {\n    @Published var participants: [Surgeon] = []\n    @Published var sharedAnnotations: [Annotation] = []\n    \n    func syncAnnotation(_ annotation: Annotation) async {\n        // SharePlay for real-time sync\n        try await groupSession?.messenger.send(annotation)\n    }\n}\n```\n\n**Key Success Metrics:**\n- Surgical planning time reduced 40%\n- Complication rates reduced 25%\n- Training effectiveness improved 60%\n- Surgeon satisfaction: 4.8/5",
      "tags": ["visionos", "healthcare", "enterprise", "3d-visualization", "hipaa"],
      "sources": ["https://developer.apple.com/visionos/"],
      "summary": "Healthcare use case: 3D surgical planning. Challenges: volumetric rendering, HIPAA compliance, precision interaction, real-time collaboration."
    },
    {
      "id": "vision_005",
      "front": "Design a real-time collaborative tool for Vision Pro (e.g., design brainstorming). How would you sync state across multiple Vision Pro devices and iOS devices?",
      "back": "**Real-Time Collaborative Design Tool:**\n\n**Architecture Overview:**\n```\n┌─────────────┐     ┌─────────────┐     ┌─────────────┐\n│ Vision Pro  │     │ Vision Pro  │     │   iPhone    │\n│  Participant│     │  Participant│     │  Participant│\n└──────┬──────┘     └──────┬──────┘     └──────┬──────┘\n       │                   │                   │\n       └───────────────────┼───────────────────┘\n                           │\n                    ┌──────▼──────┐\n                    │  SharePlay  │\n                    │   Session   │\n                    └─────────────┘\n```\n\n**SharePlay Integration:**\n```swift\nimport GroupActivities\n\n// Define the activity\nstruct DesignBrainstormActivity: GroupActivity {\n    static let activityIdentifier = \"com.app.brainstorm\"\n    \n    var metadata: GroupActivityMetadata {\n        var meta = GroupActivityMetadata()\n        meta.title = \"Design Brainstorm\"\n        meta.type = .generic\n        return meta\n    }\n}\n\n// Session Manager\nclass CollaborationManager: ObservableObject {\n    @Published var session: GroupSession<DesignBrainstormActivity>?\n    @Published var participants: [Participant] = []\n    @Published var sharedBoard: DesignBoard = DesignBoard()\n    \n    private var messenger: GroupSessionMessenger?\n    private var tasks: [Task<Void, Never>] = []\n    \n    func startSession() async {\n        for await session in DesignBrainstormActivity.sessions() {\n            self.session = session\n            self.messenger = GroupSessionMessenger(session: session)\n            \n            // Track participants\n            tasks.append(Task {\n                for await participants in session.$activeParticipants.values {\n                    await MainActor.run {\n                        self.participants = Array(participants)\n                    }\n                }\n            })\n            \n            // Receive messages\n            tasks.append(Task {\n                for await (message, _) in messenger!.messages(of: BoardUpdate.self) {\n                    await MainActor.run {\n                        self.applyUpdate(message)\n                    }\n                }\n            })\n            \n            session.join()\n        }\n    }\n    \n    func broadcastUpdate(_ update: BoardUpdate) async {\n        try? await messenger?.send(update)\n    }\n}\n```\n\n**Synced State Model:**\n```swift\n// CRDT-like conflict-free updates\nstruct BoardUpdate: Codable {\n    let id: UUID\n    let timestamp: Date\n    let participantId: UUID\n    let action: BoardAction\n}\n\nenum BoardAction: Codable {\n    case addShape(Shape)\n    case moveShape(id: UUID, position: SIMD3<Float>)\n    case deleteShape(id: UUID)\n    case addAnnotation(Annotation)\n    case updateColor(shapeId: UUID, color: CodableColor)\n}\n\nstruct DesignBoard {\n    var shapes: [UUID: Shape] = [:]\n    var annotations: [UUID: Annotation] = [:]\n    \n    mutating func apply(_ update: BoardUpdate) {\n        switch update.action {\n        case .addShape(let shape):\n            shapes[shape.id] = shape\n        case .moveShape(let id, let position):\n            shapes[id]?.position = position\n        case .deleteShape(let id):\n            shapes.removeValue(forKey: id)\n        case .addAnnotation(let annotation):\n            annotations[annotation.id] = annotation\n        case .updateColor(let shapeId, let color):\n            shapes[shapeId]?.color = color\n        }\n    }\n}\n```\n\n**Platform-Adaptive Views:**\n```swift\nstruct CollaborativeCanvas: View {\n    @StateObject var collaboration = CollaborationManager()\n    \n    var body: some View {\n        Group {\n            #if os(visionOS)\n            // 3D spatial canvas\n            RealityView { content in\n                for shape in collaboration.sharedBoard.shapes.values {\n                    let entity = createEntity(for: shape)\n                    content.add(entity)\n                }\n            } update: { content in\n                // Update entities when board changes\n                syncEntities(with: collaboration.sharedBoard, in: content)\n            }\n            .gesture(spatialDragGesture)\n            #else\n            // 2D canvas for iOS\n            Canvas { context, size in\n                for shape in collaboration.sharedBoard.shapes.values {\n                    draw2D(shape, in: context)\n                }\n            }\n            .gesture(DragGesture().onChanged { value in\n                // Convert 2D to shared coordinate space\n            })\n            #endif\n        }\n        .task { await collaboration.startSession() }\n    }\n}\n```\n\n**Conflict Resolution:**\n```swift\n// Last-write-wins with timestamp\nextension DesignBoard {\n    mutating func merge(_ update: BoardUpdate, localUpdates: [BoardUpdate]) {\n        // Check for conflicts\n        let conflicting = localUpdates.filter { local in\n            local.affectsShape(update.shapeId) && local.timestamp > update.timestamp\n        }\n        \n        if conflicting.isEmpty {\n            apply(update)\n        } else {\n            // Local change is newer, ignore remote\n            // Optionally: show conflict UI\n        }\n    }\n}\n```",
      "tags": ["visionos", "shareplay", "collaboration", "real-time", "cross-platform"],
      "sources": ["https://developer.apple.com/documentation/groupactivities"],
      "summary": "Use SharePlay for real-time sync. CRDT-like updates for conflict resolution. Platform-adaptive views (3D for visionOS, 2D for iOS). GroupSessionMessenger for state broadcast."
    }
  ]
}
